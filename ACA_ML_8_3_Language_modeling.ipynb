{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ACA ML 8.3 - Language modeling.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MelqonyanG/ML_Intro/blob/master/ACA_ML_8_3_Language_modeling.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HcMhizZjX087"
      },
      "source": [
        "# ACA ML 8.3 - Language modelling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nldAUL7JX93T"
      },
      "source": [
        "This notebook is part of the [Machine Learning Course](https://aca.am/en/machine-learning/) at Armenian Code Academy, and accompanies lectures slides and lectures nots for the 2-week block on NLP. This notebook covers the topics of language modelling with a particular focus on Recurrent Neural Networks.\n",
        "\n",
        "Created: March 25, 2021 | Last major update: March 25, 2021\n",
        "\n",
        "Author: [Vahe Tshitoyan](https://vtshitoyan.github.io) (please reach out with any questions or suggestions)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3utQ6gJbABJe",
        "outputId": "1b77473e-aeef-4d21-8513-f279d7eca094"
      },
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "import gensim\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kRcJ3yDR6G3x"
      },
      "source": [
        "You can get the corpus_100k file from [here](https://storage.googleapis.com/allnews_am/corpus_100k.zip). It is already pre-processed and tokenized."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hw9-q3hRMW7X",
        "outputId": "960c48df-4664-4872-b4fe-cbc8f58ea844"
      },
      "source": [
        "with open('corpus_100k', 'r') as f:\n",
        "  sentences = [s.strip().split(' ') for s in f.readlines()]\n",
        "print(f'Number of sentences: {len(sentences)}')\n",
        "use_first_n = 2000\n",
        "sentences = sentences[:use_first_n]\n",
        "print(f'Using: {len(sentences)}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of sentences: 1163126\n",
            "Using: 2000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QAC5-1POZbcA",
        "outputId": "ed5e34a6-155d-4bbd-b5e9-7c6750eb8e23"
      },
      "source": [
        "# Build a vocabulary from sentences\n",
        "dct = gensim.corpora.Dictionary(sentences)\n",
        "\n",
        "word2idx = dct.token2id\n",
        "idx2word = [dct[i] for i in range(len(dct.token2id))]\n",
        "idx2word = ['PAD', 'UNK'] + idx2word  # Add a token for unknown words and padding\n",
        "word2idx = {w: i for i, w in enumerate(idx2word)}\n",
        "\n",
        "print(f'Unique words: {len(idx2word)}')\n",
        "print(idx2word[:30])\n",
        "\n",
        "with open('idx2word.txt', 'w', encoding='utf-8') as idx2word_file:\n",
        "  idx2word_file.write('\\n'.join(idx2word))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Unique words: 9195\n",
            "['PAD', 'UNK', ',', '.', 'ՀՀ', 'Հիմա', 'Փաշինյան', 'ամիս', 'այդպես', 'անզգույշ', 'արտահայտեմ', 'բառ', 'ես', 'երեք', 'եւ', 'է', 'էլ', 'կարող', 'կյանքն', 'մի', 'ներքաղաքական', 'չխաղաղվի', 'քիչ', 'Մենք', 'առաջ', 'առումով', 'գնում', 'ենք', 'ճիշտ', 'վեկտորի']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FkYqmqdTcdzl"
      },
      "source": [
        "def sentence_batches(sentences, batch_size=32):\n",
        "    \"\"\"Produces training batches for the language model from sentences.\n",
        "\n",
        "    Returns data as tensorflow data.Dataset.\n",
        "\n",
        "    Args:\n",
        "      sentences: The sentences as a sequence of sequences of words.\n",
        "      batch_size: The size of the training batch.\n",
        "\n",
        "    Returns:\n",
        "      Training examples in the form of ([w1], [w2]), ([w1, w2], [w3]), etc.\n",
        "    \"\"\"\n",
        "    # Convert each sentence from list of Tokens (words) to list of word_index\n",
        "    # In the vocabulary.\n",
        "    encoded_sentences = [\n",
        "      [word2idx[w] if w in word2idx else 1 for w in s]  # 1 is for 'UNK'\n",
        "      for s in sentences\n",
        "    ]\n",
        "\n",
        "    def data_generator():\n",
        "      for encoded_sentence in encoded_sentences:\n",
        "        # Add an artificial examples for the first word after end of last \n",
        "        # sentence.\n",
        "        yield ([word2idx['։']], encoded_sentence[0])\n",
        "        for target_word_index in range(1, len(encoded_sentence)):\n",
        "          # Create the rest of the examples.\n",
        "          yield  (encoded_sentence[:target_word_index], \n",
        "                  encoded_sentence[target_word_index])\n",
        "  \n",
        "    output_signature = (\n",
        "        tf.TensorSpec(shape=(None,), dtype=tf.int32),\n",
        "        tf.TensorSpec(shape=(), dtype=tf.int32)\n",
        "    )\n",
        "\n",
        "    data = tf.data.Dataset.from_generator(\n",
        "        lambda: data_generator(),\n",
        "        output_signature=output_signature\n",
        "    ).shuffle(10000).padded_batch(batch_size)\n",
        "    return data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LU16LFe4N8ey"
      },
      "source": [
        "default_embedding_size = 50  # Used when there are no pre-trained embeddings.\n",
        "\n",
        "def rnn_model(pretrained_embeddings_file=None):\n",
        "  word_in = tf.keras.layers.Input(shape=(None,))  # The index of the word\n",
        "  if pretrained_embeddings_file is not None:\n",
        "    emb_model = gensim.models.fasttext.FastText.load(\n",
        "        pretrained_embeddings_file)\n",
        "    embedding_matrix = np.zeros((len(idx2word), emb_model.wv.vector_size))\n",
        "    for i, word in enumerate(idx2word):\n",
        "      if word in emb_model.wv.vocab:\n",
        "        embedding_matrix[i] = emb_model.wv.get_vector(word)\n",
        "      else:\n",
        "        embedding_matrix[i] = 0  # Unknown word, just use all 0s.\n",
        "    emb_word = tf.keras.layers.Embedding(\n",
        "        input_dim=len(word2idx),\n",
        "        output_dim=emb_model.wv.vector_size,\n",
        "        trainable=False,  # <- You can set this to True to fine-tune the embeddings.\n",
        "        weights=[embedding_matrix],  # <- Load pre-trained embeddings\n",
        "        mask_zero=True)(word_in)\n",
        "  else:\n",
        "    emb_word = tf.keras.layers.Embedding(\n",
        "        input_dim=len(word2idx),\n",
        "        output_dim=default_embedding_size,\n",
        "        mask_zero=True)(word_in)\n",
        "  rnn_out = tf.keras.layers.SimpleRNN(50)(emb_word)\n",
        "  predicted_word = tf.keras.layers.Dense(\n",
        "      len(idx2word), activation = 'softmax')(rnn_out)\n",
        "  return tf.keras.models.Model(word_in, predicted_word)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ETJnTElUhsFs"
      },
      "source": [
        "model = rnn_model('ft_50_1679k_and_wiki_lr0025_cn36_ss000001.model')\n",
        "model.compile(loss=tf.keras.losses.SparseCategoricalCrossentropy(), \n",
        "              optimizer=tf.keras.optimizers.Adam())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KcMirEhihzhu",
        "outputId": "57794791-76f7-4269-e35e-5d8f613943a5"
      },
      "source": [
        "_ = model.fit(\n",
        "    sentence_batches(sentences, batch_size=32), \n",
        "    epochs=30)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/30\n",
            "1377/1377 [==============================] - 76s 54ms/step - loss: 2.6783\n",
            "Epoch 2/30\n",
            "1377/1377 [==============================] - 76s 54ms/step - loss: 2.6733\n",
            "Epoch 3/30\n",
            "1377/1377 [==============================] - 74s 53ms/step - loss: 2.6652\n",
            "Epoch 4/30\n",
            "1377/1377 [==============================] - 76s 54ms/step - loss: 2.6590\n",
            "Epoch 5/30\n",
            "1377/1377 [==============================] - 75s 54ms/step - loss: 2.6495\n",
            "Epoch 6/30\n",
            "1377/1377 [==============================] - 75s 54ms/step - loss: 2.6415\n",
            "Epoch 7/30\n",
            "1377/1377 [==============================] - 76s 54ms/step - loss: 2.6358\n",
            "Epoch 8/30\n",
            "1377/1377 [==============================] - 76s 54ms/step - loss: 2.6249\n",
            "Epoch 9/30\n",
            "1377/1377 [==============================] - 76s 54ms/step - loss: 2.6195\n",
            "Epoch 10/30\n",
            "1377/1377 [==============================] - 76s 54ms/step - loss: 2.6112\n",
            "Epoch 11/30\n",
            "1377/1377 [==============================] - 74s 53ms/step - loss: 2.6035\n",
            "Epoch 12/30\n",
            "1377/1377 [==============================] - 76s 54ms/step - loss: 2.5982\n",
            "Epoch 13/30\n",
            "1377/1377 [==============================] - 75s 54ms/step - loss: 2.5901\n",
            "Epoch 14/30\n",
            "1377/1377 [==============================] - 75s 54ms/step - loss: 2.5855\n",
            "Epoch 15/30\n",
            "1377/1377 [==============================] - 75s 54ms/step - loss: 2.5852\n",
            "Epoch 16/30\n",
            "1377/1377 [==============================] - 75s 53ms/step - loss: 2.5810\n",
            "Epoch 17/30\n",
            "1377/1377 [==============================] - 75s 53ms/step - loss: 2.5662\n",
            "Epoch 18/30\n",
            "1377/1377 [==============================] - 76s 54ms/step - loss: 2.5621\n",
            "Epoch 19/30\n",
            "1377/1377 [==============================] - 74s 53ms/step - loss: 2.5597\n",
            "Epoch 20/30\n",
            "1377/1377 [==============================] - 75s 53ms/step - loss: 2.5625\n",
            "Epoch 21/30\n",
            "1377/1377 [==============================] - 74s 53ms/step - loss: 2.5441\n",
            "Epoch 22/30\n",
            "1377/1377 [==============================] - 76s 54ms/step - loss: 2.5351\n",
            "Epoch 23/30\n",
            "1377/1377 [==============================] - 76s 54ms/step - loss: 2.5334\n",
            "Epoch 24/30\n",
            "1377/1377 [==============================] - 76s 54ms/step - loss: 2.5267\n",
            "Epoch 25/30\n",
            "1377/1377 [==============================] - 76s 54ms/step - loss: 2.5266\n",
            "Epoch 26/30\n",
            "1377/1377 [==============================] - 75s 53ms/step - loss: 2.5185\n",
            "Epoch 27/30\n",
            "1377/1377 [==============================] - 76s 54ms/step - loss: 2.5150\n",
            "Epoch 28/30\n",
            "1377/1377 [==============================] - 75s 54ms/step - loss: 2.5108\n",
            "Epoch 29/30\n",
            "1377/1377 [==============================] - 78s 55ms/step - loss: 2.5104\n",
            "Epoch 30/30\n",
            "1377/1377 [==============================] - 77s 55ms/step - loss: 2.5005\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gPKY5Fa-vpqI"
      },
      "source": [
        "save_model = True\n",
        "if save_model:\n",
        "  model.save('arm_rnn_language_model.h5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2R9MOicG7l2L"
      },
      "source": [
        "# Text generation using the trained model\n",
        "You can run this without the training part above if you have already saved the idx2word.txt (the vocabulary) and the arm_rnn_language_model.h5 (the language model)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gEaXXWeJzDPy"
      },
      "source": [
        "model = tf.keras.models.load_model('arm_rnn_language_model.h5')\n",
        "with open('idx2word.txt', 'r', encoding='utf-8') as idx2word_file:\n",
        "  idx2word = [w.strip() for w in idx2word_file.readlines()]\n",
        "word2idx = {w: i for i, w in enumerate(idx2word)}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z8Y4_z88iTfg"
      },
      "source": [
        "def generate_text(input_sentence, nr_extra_words):\n",
        "  # Converts words to their indexes in the vocabulary.\n",
        "  encoded_input = [word2idx[w] if w in word2idx else 1 for w in input_sentence]\n",
        "  for _ in range(nr_extra_words):\n",
        "    next_word_distribution = model.predict([encoded_input])[0]\n",
        "    # Sample a word using the output distribution.\n",
        "    next_word = np.random.choice(\n",
        "        range(len(idx2word)), \n",
        "        p=next_word_distribution)\n",
        "    # Add the new word to the sequence to continue generating text.\n",
        "    encoded_input.append(next_word)\n",
        "  return [idx2word[encoded_word] for encoded_word in encoded_input]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86
        },
        "id": "ALhiqVTGkFr_",
        "outputId": "6b9f9527-971d-4686-e6d0-942f5b95002c"
      },
      "source": [
        "' '.join(generate_text(['ՀՀ', 'քաղաքացիները'], 50))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'ՀՀ քաղաքացիները , անհանդուրժողականությունը . ՀՔԾ օրենսգրքի 178-րդ հոդվածի 2-րդ մասի հատկանիշներով 5 մահապարտ են տարել ։ Սարավանի լեռնանցքում գետնաբուք այսօրվա . Ռոբերտ Քոչարյանի պարագայում մերսման համար ։ Ու թող դրան ընթացքում ՝ նախարարի առաջին պայթյունը մի քանի օր է տեղում ։ 00-ը - նշեց արդարադատության նախարարի նորանշանակ աշխատակազմի ղեկավարի Վլադիմիր Զելենսկին'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s-aeMO30jf0H"
      },
      "source": [
        "# Improvement ideas\n",
        "- Use more RNN layers\n",
        "- Use more modern Recurrent layers such as LSTMs.\n",
        "- Use beam search instead of sampling.\n",
        "- Increase the dimensions of the hidden layer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9mhmkrBClnKq"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}